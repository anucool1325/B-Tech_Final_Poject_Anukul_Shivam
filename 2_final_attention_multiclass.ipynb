{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imresize' from 'scipy.misc' (/Users/anukulgaurav/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/misc/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b182b6cf187e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_saliency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/vis/visualization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mactivation_maximization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msaliency\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_saliency_with_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msaliency\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_saliency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msaliency\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_cam_with_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/vis/visualization/saliency.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'imresize' from 'scipy.misc' (/Users/anukulgaurav/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/scipy/misc/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16 as PTModel\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda, AvgPool2D\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "from vis.visualization import visualize_saliency\n",
    "from vis.utils import utils\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below codes for downloading the data\n",
    "\n",
    "#!gdown https://drive.google.com/open?id=1odxJF4kyHEtBqhkvz3iXpV3iQK34m6z0\n",
    "#!unzip covid_data_compiled_sagar.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global variables\n",
    "data_list = os.listdir('multi_class/train')\n",
    "DATASET_PATH  = 'multi_class/train'\n",
    "test_dir =  'multi_class/test'\n",
    "IMAGE_SIZE    = (150, 150)\n",
    "NUM_CLASSES   = len(data_list)\n",
    "BATCH_SIZE    = 10  # try reducing batch size or freeze more layers if your GPU runs out of memory\n",
    "NUM_EPOCHS    = 80\n",
    "LEARNING_RATE =0.00001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(training=True):\n",
    "    \n",
    "\n",
    "    # Train Image Augmentation\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                       rotation_range=50,\n",
    "                                       featurewise_center = True,\n",
    "                                       featurewise_std_normalization = True,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       shear_range=0.25,\n",
    "                                       zoom_range=0.1,\n",
    "                                       zca_whitening = True,\n",
    "                                       channel_shift_range = 20,\n",
    "                                       horizontal_flip = True ,\n",
    "                                       vertical_flip = True ,\n",
    "                                       validation_split = 0.2,\n",
    "                                       fill_mode='constant')\n",
    "\n",
    "    \n",
    "    if training == True:\n",
    "        \n",
    "\n",
    "        batches = train_datagen.flow_from_directory(DATASET_PATH,\n",
    "                                                          target_size=IMAGE_SIZE,\n",
    "                                                          shuffle=True,\n",
    "                                                          batch_size=BATCH_SIZE,\n",
    "                                                          subset = \"training\",\n",
    "                                                          seed=42,\n",
    "                                                          class_mode=\"categorical\"\n",
    "                                                          )\n",
    "    else:\n",
    "        \n",
    "        batches = train_datagen.flow_from_directory(DATASET_PATH,\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  subset = \"validation\",\n",
    "                                                  seed=42,\n",
    "                                                  class_mode=\"categorical\"\n",
    "                                                  )\n",
    "        \n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model architecture and using VGG 16 with Imagenet weights as initial layer and using attention as an additive model.\n",
    "\n",
    "We don't want to do any pooling since some parts of the image are more important than the others. So we build an attention mechanism to turn pixels in the GAP on an off. Then the pooling and then rescale (Lambda layer) happens based on the number of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention(learning_rate = LEARNING_RATE):\n",
    "    base_pretrained_model = PTModel(input_shape =  (150, 150, 3), \n",
    "                              include_top = False, weights = 'imagenet')\n",
    "    base_pretrained_model.trainable = False\n",
    "    model = models.Sequential()\n",
    "    pt_features = Input(base_pretrained_model.get_output_shape_at(0)[1:], name = 'feature_input')\n",
    "    pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n",
    "    bn_features = BatchNormalization()(pt_features)\n",
    "    # here we do an attention mechanism to turn pixels in the GAP on an off\n",
    "    attn_layer = Conv2D(128, kernel_size = (1,1), padding = 'same', activation = 'elu')(bn_features)\n",
    "    attn_layer = Conv2D(32, kernel_size = (1,1), padding = 'same', activation = 'elu')(attn_layer)\n",
    "    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'elu')(attn_layer)\n",
    "    attn_layer = AvgPool2D((2,2), strides = (1,1), padding = 'same')(attn_layer) # smooth results\n",
    "    attn_layer = Conv2D(1, \n",
    "                        kernel_size = (1,1), \n",
    "                        padding = 'valid', \n",
    "                        activation = 'sigmoid')(attn_layer)\n",
    "    # branch it to all channel\n",
    "    up_c2_w = np.ones((1, 1, 1, pt_depth))\n",
    "    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n",
    "                   activation = 'linear', use_bias = False, weights = [up_c2_w])\n",
    "    up_c2.trainable = False\n",
    "    attn_layer = up_c2(attn_layer)\n",
    "    mask_features = multiply([attn_layer, bn_features])\n",
    "    gap_features = GlobalAveragePooling2D()(mask_features)\n",
    "    gap_mask = GlobalAveragePooling2D()(attn_layer)\n",
    "    # to account for missing values from the attention model\n",
    "    gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n",
    "    gap_dr = Dropout(0.5)(gap)\n",
    "    dr_steps = Dropout(0.5)(Dense(128, activation = 'elu')(gap_dr))\n",
    "    out_layer = Dense(3, activation = 'softmax')(dr_steps)\n",
    "    attn_model = Model(inputs = [pt_features], outputs = [out_layer], name = 'attention_model')\n",
    "\n",
    "    attn_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                               metrics = ['acc'])\n",
    "    \n",
    "    print('Summary of Attention model only: ')\n",
    "    print(attn_model.summary())\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    tb_model = Sequential(name = 'combined_model')\n",
    "    base_pretrained_model.trainable = False\n",
    "    tb_model.add(base_pretrained_model)\n",
    "    tb_model.add(attn_model)\n",
    "    tb_model.compile(optimizer = Adam(lr = 1e-3), loss = 'categorical_crossentropy',\n",
    "                               metrics = ['acc'])\n",
    "    print(\"Summary of final model: \")\n",
    "    print(tb_model.summary())\n",
    "    \n",
    "    return tb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback():\n",
    "    weight_path=\"{}_weights.best.hdf5\".format('covid_attn')\n",
    "    checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=5, min_lr=0.0001)\n",
    "    '''early = EarlyStopping(monitor=\"val_loss\", \n",
    "                          mode=\"min\", \n",
    "                          patience=10)''' # probably needs to be more patient, but kaggle time is limited\n",
    "    callbacks_list = [checkpoint, reduceLROnPlat]\n",
    "    \n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, train_batches, valid_batches, callbacks_list):\n",
    "    \n",
    "    STEP_SIZE_TRAIN=train_batches.n//train_batches.batch_size\n",
    "    STEP_SIZE_VALID=valid_batches.n//valid_batches.batch_size\n",
    "    \n",
    "    result = model.fit_generator(train_batches, \n",
    "                       steps_per_epoch =STEP_SIZE_TRAIN,\n",
    "                      validation_data = valid_batches, \n",
    "                       validation_steps = STEP_SIZE_VALID,\n",
    "                      epochs = 100, \n",
    "                      callbacks = callbacks_list,\n",
    "                      workers = 3)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(result, epochs = 100):\n",
    "\n",
    "    acc = result.history['acc']\n",
    "    loss = result.history['loss']\n",
    "    val_acc = result.history['val_acc']\n",
    "    val_loss = result.history['val_loss']\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(range(1,epochs), acc[1:], label='Train_acc')\n",
    "    plt.plot(range(1,epochs), val_acc[1:], label='Test_acc')\n",
    "    plt.title('Accuracy over ' + str(epochs) + ' Epochs', size=15)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.subplot(122)\n",
    "    plt.plot(range(1,epochs), loss[1:], label='Train_loss')\n",
    "    plt.plot(range(1,epochs), val_loss[1:], label='Test_loss')\n",
    "    plt.title('Loss over ' + str(epochs) + ' Epochs', size=15)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test_data_and_image_map(model, test_dir=test_dir, target_size=IMAGE_SIZE, batch_size = BATCH_SIZE):\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    eval_generator = test_datagen.flow_from_directory(\n",
    "     test_dir,target_size=IMAGE_SIZE,\n",
    "     batch_size=1,\n",
    "     shuffle=False,\n",
    "     seed=42, \n",
    "     class_mode='categorical')\n",
    "\n",
    "    eval_generator.reset()\n",
    "    x = model.evaluate_generator(eval_generator,\n",
    "     steps = np.ceil(len(eval_generator) / BATCH_SIZE),\n",
    "     use_multiprocessing = False,\n",
    "     verbose = 1,\n",
    "     workers=1\n",
    "     )\n",
    "    print('Test loss:' , x[0])\n",
    "    print('Test accuracy:',x[1])\n",
    "    \n",
    "    eval_generator.reset()  \n",
    "    pred = model.predict_generator(eval_generator,1000,verbose=1)\n",
    "    \n",
    "    for index, probability in enumerate(pred):\n",
    "        try:\n",
    "            image_path = test_dir + \"/\" +eval_generator.filenames[index]\n",
    "            image = mpimg.imread(image_path)\n",
    "            #BGR TO RGB conversion using CV2\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            pixels = np.array(image)\n",
    "            plt.imshow(pixels)\n",
    "\n",
    "            print(eval_generator.filenames[index])\n",
    "            if np.argmax(probability) == 0:\n",
    "                plt.title(\"%.2f\" % (probability[0]*100) + \"% COVID19\")\n",
    "            elif np.argmax(probability) == 1:\n",
    "                plt.title(\"%.2f\" % (probability[1]*100) + \"% Normal\")\n",
    "            else:\n",
    "                plt.title(\"%.2f\" % (probability[2]*100) + \"% Tertiary pneumonia\")\n",
    "            plt.show()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_batches = augment()\n",
    "    valid_batches = augment(False)\n",
    "    model = model_attention()\n",
    "    callback_list = callback()\n",
    "    result = fit_model(model, train_batches, valid_batches, callback_list)\n",
    "    plot_results(result, epochs = 100)\n",
    "    check_test_data_and_image_map(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
